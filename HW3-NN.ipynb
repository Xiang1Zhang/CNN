{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Mining: Assignment Resit NN\n",
    "\n",
    "Please complete all assignments in this notebook. You should submit this notebook, as well as a PDF version (See File > Download as)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully convolutional neural networks\n",
    "\n",
    "The goal of this excercise is to develop a model for recognizing seqences of MNIST digits in an image. You are given a image of size $(28 by 28*n)$ where $n$ is also an input parameter. The model needs to be able to recognize the sequence of digits in the image. \n",
    "\n",
    "Training many different models for different values of $n$ is not very efficient. A more efficient strategy is to develop a model for detecting single digits and apply that model on the input image by sliding it (to cover the larger surface) and then post processing its output. Sliding a CNN model over a large image in the general case results in re-computing many of the computations. One approach that deals with this efficiently is converting the CNN model into a fully convolutional neural network(*).\n",
    "\n",
    "Any Dense layer can be converted to a Convolutional layer. For example, a Dense layer with K=128 neurons that inputs an activation map 7×7×512 can be equivalently expressed as a Convolutional layer with filter size of 7x7, (padding = 0, stride 1x1) and K=128 number of filters. Rather than flattening the activation map and feeding it into a dense layer we are using a Convolutional layer and setting the filter size to be exactly the size of the input volume (For more detais: https://arxiv.org/pdf/1411.4038.pdf).\n",
    "\n",
    "To solve this excercise extend/modify the code given bellow to:\n",
    "* Produce a CNN model for single digit detection of MNIST images\n",
    "* Convert this model into a fully convolutional neural network (FCNN)\n",
    "* Run the FCNN model on a image that is a concatenation of a sequence of MNIST images of length n=5\n",
    "* Post-process the FCNN output from the previous point to produce a numerical sequence of digits as output\n",
    "* Discuss how your implementation would need to be modified if the input is not a concatenation of MNIST images but rather the MNIST images are pasted on a random location on a large canvas (no implementatation is necessary)\n",
    "\n",
    "(*) FCNN models are very useful for processing very large images, such as produced in digital pathology or satelite imaging. This models are applicable to any domain where the size of the objects in the image is much smaller than the image itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Convolution2D\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import random\n",
    "# Training parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_digits = 5\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "    input_shape_test = (1, img_rows, img_cols*num_digits)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    input_shape_test = (img_rows, img_cols * num_digits, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model definition\n",
    "def build_cnn_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    # <<<<<<<<<<<<<<<<<<\n",
    "    # Add convolutional layers here to finalize the CNN model\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # <<<<<<<<<<<<<<<<<<\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCNN Model definition\n",
    "def build_full_conv_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    # <<<<<<<<<<<<<<<<<<<\n",
    "    # Add convolutional layers here that match the CNN architecture\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape_test))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # <<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    # This is are two example Conv layers that do the same operations \n",
    "    # as the Dense layers with 128 neurons and the final classification layer as given in the CNN model \n",
    "    # replace the <q> value of the size of the filters, such that it matches the dimension of the activation map\n",
    "    model.add(Conv2D(128, kernel_size=(12, 12),strides=14, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(num_classes, kernel_size=(1, 1)))\n",
    "    model.add(Reshape((-1, num_classes)))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transplant_weight(weights_in, weights_out):\n",
    "    for layer in range(len(weights_in)):\n",
    "        weights_out[layer].flat = weights_in[layer].flat\n",
    "    return weights_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_concat_mnist(images, labels, size=100, length=5):\n",
    "    # form the appropriate tensor for the result\n",
    "    image_width = images.shape[2]\n",
    "    out = np.zeros((size, images.shape[1], image_width*length, images.shape[3]))\n",
    "    out_labels = np.zeros((size, length, labels.shape[1]))\n",
    "    # randomly sample images (size x length samples)\n",
    "    indexes = np.random.randint(0, images.shape[0], (size, length))\n",
    "    # fill in the final tensor for images and labels\n",
    "    for i in range(size):\n",
    "        for j in range(length):\n",
    "            start = image_width*j\n",
    "            end = image_width*(j+1)\n",
    "            out[i, :, start:end] = images[indexes[i, j]]\n",
    "            out_labels[i, j] = labels[indexes[i, j]]\n",
    "    return out, out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Xiang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Xiang\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Xiang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.2755 - acc: 0.9145 - val_loss: 0.0597 - val_acc: 0.9809\n",
      "Model output\n",
      "[[3 8 7 2 7]\n",
      " [1 5 5 6 0]\n",
      " [8 1 2 3 1]\n",
      " [9 9 2 4 1]\n",
      " [5 2 9 9 7]\n",
      " [3 0 6 4 1]\n",
      " [9 4 9 6 1]\n",
      " [9 2 7 7 2]\n",
      " [4 2 3 2 6]\n",
      " [1 3 9 4 8]\n",
      " [8 8 2 4 3]\n",
      " [2 6 8 1 4]\n",
      " [6 3 4 0 0]\n",
      " [8 1 5 7 4]\n",
      " [2 8 7 5 3]\n",
      " [5 7 1 1 3]\n",
      " [4 3 4 7 6]\n",
      " [2 8 0 7 4]\n",
      " [1 4 1 7 5]\n",
      " [2 3 7 6 6]\n",
      " [9 3 0 6 7]\n",
      " [7 0 9 0 0]\n",
      " [2 2 1 2 6]\n",
      " [2 4 6 4 6]\n",
      " [2 7 4 7 2]\n",
      " [2 2 2 7 7]\n",
      " [5 1 9 9 2]\n",
      " [3 4 2 1 2]\n",
      " [9 5 9 1 5]\n",
      " [7 9 5 3 7]\n",
      " [6 3 5 1 9]\n",
      " [0 1 5 4 3]\n",
      " [0 9 2 8 1]\n",
      " [8 2 3 7 9]\n",
      " [6 4 6 2 6]\n",
      " [7 6 8 2 8]\n",
      " [3 0 0 3 8]\n",
      " [6 1 7 6 8]\n",
      " [9 7 2 8 2]\n",
      " [1 4 3 3 8]\n",
      " [1 6 0 8 7]\n",
      " [7 2 2 6 6]\n",
      " [5 3 8 6 3]\n",
      " [1 0 4 6 0]\n",
      " [1 8 7 9 6]\n",
      " [4 9 8 2 0]\n",
      " [8 7 2 3 3]\n",
      " [3 4 1 1 6]\n",
      " [1 1 0 6 0]\n",
      " [6 6 7 6 3]\n",
      " [4 7 0 1 9]\n",
      " [3 1 8 3 0]\n",
      " [4 9 2 4 0]\n",
      " [3 7 3 3 2]\n",
      " [6 0 2 9 1]\n",
      " [2 4 6 3 8]\n",
      " [1 6 8 2 1]\n",
      " [0 8 1 1 6]\n",
      " [6 7 1 8 4]\n",
      " [2 5 9 9 9]\n",
      " [4 5 0 6 3]\n",
      " [7 0 8 5 3]\n",
      " [5 9 0 4 8]\n",
      " [5 0 2 7 6]\n",
      " [4 0 4 0 5]\n",
      " [4 5 1 6 3]\n",
      " [5 6 3 2 4]\n",
      " [4 2 1 6 9]\n",
      " [6 9 5 6 8]\n",
      " [2 2 0 9 8]\n",
      " [0 1 7 9 4]\n",
      " [3 2 7 2 9]\n",
      " [9 7 0 3 8]\n",
      " [7 5 1 5 8]\n",
      " [0 3 9 1 8]\n",
      " [2 8 3 7 6]\n",
      " [2 1 5 7 2]\n",
      " [2 6 5 9 8]\n",
      " [7 0 9 2 2]\n",
      " [0 8 4 3 2]\n",
      " [5 2 3 2 2]\n",
      " [8 7 3 9 2]\n",
      " [9 9 8 7 1]\n",
      " [8 6 8 8 5]\n",
      " [3 8 2 2 5]\n",
      " [8 8 1 0 4]\n",
      " [5 7 0 3 9]\n",
      " [1 1 7 5 6]\n",
      " [7 3 8 9 0]\n",
      " [8 3 4 4 8]\n",
      " [3 8 7 5 6]\n",
      " [5 5 7 4 3]\n",
      " [6 6 9 2 8]\n",
      " [2 4 8 5 8]\n",
      " [6 2 2 3 3]\n",
      " [1 8 4 8 7]\n",
      " [3 4 9 2 0]\n",
      " [8 0 6 5 9]\n",
      " [9 2 4 0 7]\n",
      " [9 6 4 6 8]]\n",
      "Label outputs\n",
      "[[3 8 7 2 7]\n",
      " [1 5 5 6 0]\n",
      " [8 1 2 3 1]\n",
      " [9 9 2 4 1]\n",
      " [5 2 9 9 7]\n",
      " [3 0 6 4 1]\n",
      " [9 4 9 6 1]\n",
      " [9 2 7 7 2]\n",
      " [4 2 3 2 6]\n",
      " [1 3 9 4 8]\n",
      " [8 8 2 4 3]\n",
      " [2 6 8 1 4]\n",
      " [6 3 4 0 0]\n",
      " [8 1 5 7 4]\n",
      " [2 8 7 5 3]\n",
      " [5 7 1 1 3]\n",
      " [4 3 4 7 6]\n",
      " [2 8 0 7 4]\n",
      " [1 4 1 7 5]\n",
      " [2 3 7 6 6]\n",
      " [9 3 0 6 7]\n",
      " [7 0 9 0 0]\n",
      " [2 2 1 2 6]\n",
      " [2 4 6 4 6]\n",
      " [2 7 4 7 2]\n",
      " [2 2 2 7 7]\n",
      " [5 1 9 9 2]\n",
      " [3 4 2 1 2]\n",
      " [9 5 9 1 5]\n",
      " [7 9 5 3 7]\n",
      " [6 3 5 1 9]\n",
      " [0 1 5 4 3]\n",
      " [0 9 2 8 1]\n",
      " [8 2 3 7 9]\n",
      " [6 4 6 2 6]\n",
      " [7 6 8 2 8]\n",
      " [3 0 0 3 8]\n",
      " [6 1 7 6 8]\n",
      " [9 7 2 8 2]\n",
      " [1 4 5 3 8]\n",
      " [1 6 0 8 7]\n",
      " [7 2 2 6 6]\n",
      " [5 3 8 6 3]\n",
      " [1 0 4 6 0]\n",
      " [1 8 7 9 6]\n",
      " [4 9 8 7 0]\n",
      " [8 7 2 3 3]\n",
      " [3 4 1 1 6]\n",
      " [1 1 0 6 0]\n",
      " [6 6 7 6 3]\n",
      " [4 8 0 1 9]\n",
      " [3 1 8 3 0]\n",
      " [4 9 2 4 0]\n",
      " [3 7 3 3 2]\n",
      " [6 0 2 9 1]\n",
      " [2 4 6 3 8]\n",
      " [1 6 8 2 1]\n",
      " [8 8 1 1 6]\n",
      " [6 7 1 8 4]\n",
      " [2 5 9 9 9]\n",
      " [4 5 0 6 2]\n",
      " [7 0 8 5 3]\n",
      " [5 9 0 4 8]\n",
      " [5 0 2 7 6]\n",
      " [4 0 4 0 1]\n",
      " [4 5 1 6 3]\n",
      " [5 6 3 7 4]\n",
      " [4 2 1 6 9]\n",
      " [6 9 5 6 8]\n",
      " [2 2 0 9 8]\n",
      " [0 1 7 9 4]\n",
      " [3 2 7 2 9]\n",
      " [9 7 0 3 8]\n",
      " [7 9 1 5 8]\n",
      " [0 3 9 1 8]\n",
      " [2 8 3 7 6]\n",
      " [2 1 5 7 2]\n",
      " [2 6 5 4 8]\n",
      " [7 0 9 2 2]\n",
      " [0 8 4 3 2]\n",
      " [9 2 3 2 2]\n",
      " [8 7 3 9 2]\n",
      " [9 9 8 7 1]\n",
      " [8 6 8 8 5]\n",
      " [3 8 7 2 5]\n",
      " [8 8 1 0 4]\n",
      " [5 7 0 3 9]\n",
      " [1 1 7 5 6]\n",
      " [7 3 8 9 0]\n",
      " [8 3 4 4 8]\n",
      " [3 8 7 5 6]\n",
      " [5 5 7 4 3]\n",
      " [6 6 9 2 8]\n",
      " [2 4 8 5 8]\n",
      " [6 2 2 5 3]\n",
      " [1 8 4 8 7]\n",
      " [3 4 9 2 0]\n",
      " [8 0 6 5 9]\n",
      " [9 2 4 0 7]\n",
      " [9 6 4 6 8]]\n"
     ]
    }
   ],
   "source": [
    "# build test data\n",
    "(x_concat_test, y_concat_test) = build_concat_mnist(x_test, y_test)\n",
    "\n",
    "# build models\n",
    "model = build_cnn_model()\n",
    "full_conv_model = build_full_conv_model()\n",
    "\n",
    "# model file\n",
    "model_file = \"mnist-model-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "# train\n",
    "model.fit(x_train, y_train,batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "#model = train_cnn_model(model_file, model)\n",
    "# During testing you can load the CNN parameters, rather than train each time\n",
    "#model_load = \"mnist-model.hdf5\"\n",
    "#model.load_weights(model_load)\n",
    "\n",
    "model_weights = model.get_weights()\n",
    "\n",
    "full_conv_weights = full_conv_model.get_weights()\n",
    "full_conv_weights = transplant_weight(model_weights, full_conv_weights)\n",
    "full_conv_model.set_weights(full_conv_weights)\n",
    "\n",
    "# Execute the FCNN\n",
    "output = full_conv_model.predict(x_concat_test, batch_size=batch_size)\n",
    "# The output of the fully conv model needs to be processed to produce the sequence of digits\n",
    "output = np.argmax(output, axis=2)\n",
    "print(\"Model output\")\n",
    "# <<<<<<<<<<<<<<<\n",
    "# post processing the model output\n",
    "print(output)\n",
    "# <<<<<<<<<<<<<<<\n",
    "\n",
    "\n",
    "# the true labels for comparison\n",
    "labels = np.argmax(y_concat_test, axis=2)\n",
    "print(\"Label outputs\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(0,100):\n",
    "    for j in range(0,5):\n",
    "        if(output[i][j]==labels[i][j]):\n",
    "            count = count+1\n",
    "            \n",
    "accuracy = count/500\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
